# 监控配置文件
# 包含Prometheus、Grafana、AlertManager等监控组件配置

# Prometheus配置
prometheus:
  global:
    scrape_interval: 15s
    evaluation_interval: 15s
    external_labels:
      cluster: 'wlweb'
      environment: '${ENVIRONMENT:-development}'
  
  # 规则文件
  rule_files:
    - "rules/*.yml"
  
  # 告警管理器配置
  alerting:
    alertmanagers:
      - static_configs:
          - targets:
              - alertmanager:9093
  
  # 抓取配置
  scrape_configs:
    # Prometheus自身监控
    - job_name: 'prometheus'
      static_configs:
        - targets: ['localhost:9090']
    
    # Node Exporter（系统指标）
    - job_name: 'node-exporter'
      static_configs:
        - targets: ['node-exporter:9100']
      scrape_interval: 30s
    
    # cAdvisor（容器指标）
    - job_name: 'cadvisor'
      static_configs:
        - targets: ['cadvisor:8080']
      scrape_interval: 30s
    
    # MySQL Exporter
    - job_name: 'mysql-exporter'
      static_configs:
        - targets: ['mysql-exporter:9104']
      scrape_interval: 30s
    
    # Redis Exporter
    - job_name: 'redis-exporter'
      static_configs:
        - targets: ['redis-exporter:9121']
      scrape_interval: 30s
    
    # FastAPI应用监控
    - job_name: 'fastapi-app'
      static_configs:
        - targets: ['backend:8000']
      metrics_path: '/metrics'
      scrape_interval: 15s
    
    # Nginx监控
    - job_name: 'nginx'
      static_configs:
        - targets: ['nginx-exporter:9113']
      scrape_interval: 30s
    
    # Blackbox Exporter（端点监控）
    - job_name: 'blackbox'
      metrics_path: /probe
      params:
        module: [http_2xx]
      static_configs:
        - targets:
            - http://frontend:80
            - http://backend:8000/health
            - http://backend:8000/api/v1/health
      relabel_configs:
        - source_labels: [__address__]
          target_label: __param_target
        - source_labels: [__param_target]
          target_label: instance
        - target_label: __address__
          replacement: blackbox-exporter:9115

# Grafana配置
grafana:
  # 数据源配置
  datasources:
    - name: Prometheus
      type: prometheus
      access: proxy
      url: http://prometheus:9090
      isDefault: true
    
    - name: Loki
      type: loki
      access: proxy
      url: http://loki:3100
  
  # 仪表板配置
  dashboards:
    - name: "System Overview"
      path: "/var/lib/grafana/dashboards/system-overview.json"
    
    - name: "Application Metrics"
      path: "/var/lib/grafana/dashboards/app-metrics.json"
    
    - name: "Database Performance"
      path: "/var/lib/grafana/dashboards/db-performance.json"
    
    - name: "Container Monitoring"
      path: "/var/lib/grafana/dashboards/container-monitoring.json"
  
  # 插件配置
  plugins:
    - grafana-piechart-panel
    - grafana-worldmap-panel
    - grafana-clock-panel

# AlertManager配置
alertmanager:
  global:
    smtp_smarthost: '${SMTP_HOST:-localhost:587}'
    smtp_from: 'alerts@wlweb.local'
  
  # 路由配置
  route:
    group_by: ['alertname', 'cluster', 'service']
    group_wait: 10s
    group_interval: 10s
    repeat_interval: 1h
    receiver: 'web.hook'
    routes:
      - match:
          severity: critical
        receiver: 'critical-alerts'
      - match:
          severity: warning
        receiver: 'warning-alerts'
  
  # 接收器配置
  receivers:
    - name: 'web.hook'
      webhook_configs:
        - url: '${WEBHOOK_URL:-http://localhost:5001/webhook}'
    
    - name: 'critical-alerts'
      email_configs:
        - to: 'admin@wlweb.local'
          subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
          body: |
            {{ range .Alerts }}
            Alert: {{ .Annotations.summary }}
            Description: {{ .Annotations.description }}
            {{ end }}
      slack_configs:
        - api_url: '${SLACK_WEBHOOK_URL}'
          channel: '#critical-alerts'
          title: '[CRITICAL] {{ .GroupLabels.alertname }}'
          text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    
    - name: 'warning-alerts'
      email_configs:
        - to: 'ops@wlweb.local'
          subject: '[WARNING] {{ .GroupLabels.alertname }}'
          body: |
            {{ range .Alerts }}
            Alert: {{ .Annotations.summary }}
            Description: {{ .Annotations.description }}
            {{ end }}

# 告警规则
alert_rules:
  # 系统资源告警
  system:
    - alert: HighCPUUsage
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage detected"
        description: "CPU usage is above 80% for more than 5 minutes"
    
    - alert: HighMemoryUsage
      expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage detected"
        description: "Memory usage is above 85% for more than 5 minutes"
    
    - alert: DiskSpaceLow
      expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Disk space is running low"
        description: "Available disk space is less than 10%"
  
  # 应用程序告警
  application:
    - alert: HighErrorRate
      expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) * 100 > 5
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "High error rate detected"
        description: "Error rate is above 5% for more than 5 minutes"
    
    - alert: SlowResponseTime
      expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Slow response time detected"
        description: "95th percentile response time is above 2 seconds"
    
    - alert: ServiceDown
      expr: up == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Service is down"
        description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute"
  
  # 数据库告警
  database:
    - alert: MySQLDown
      expr: mysql_up == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "MySQL is down"
        description: "MySQL database is not responding"
    
    - alert: MySQLSlowQueries
      expr: rate(mysql_global_status_slow_queries[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "MySQL slow queries detected"
        description: "MySQL slow query rate is above 0.1 per second"
    
    - alert: RedisDown
      expr: redis_up == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Redis is down"
        description: "Redis cache is not responding"

# 监控指标配置
metrics:
  # 应用指标
  application:
    - name: "http_requests_total"
      help: "Total number of HTTP requests"
      type: "counter"
      labels: ["method", "endpoint", "status"]
    
    - name: "http_request_duration_seconds"
      help: "HTTP request duration in seconds"
      type: "histogram"
      labels: ["method", "endpoint"]
      buckets: [0.1, 0.5, 1.0, 2.0, 5.0]
    
    - name: "active_users"
      help: "Number of active users"
      type: "gauge"
    
    - name: "database_connections"
      help: "Number of database connections"
      type: "gauge"
      labels: ["state"]
  
  # 业务指标
  business:
    - name: "user_registrations_total"
      help: "Total number of user registrations"
      type: "counter"
    
    - name: "orders_total"
      help: "Total number of orders"
      type: "counter"
      labels: ["status"]
    
    - name: "revenue_total"
      help: "Total revenue"
      type: "counter"

# 健康检查配置
health_checks:
  endpoints:
    - name: "backend-health"
      url: "http://backend:8000/health"
      interval: 30s
      timeout: 5s
      expected_status: 200
    
    - name: "frontend-health"
      url: "http://frontend:80"
      interval: 30s
      timeout: 5s
      expected_status: 200
    
    - name: "database-health"
      url: "http://backend:8000/api/v1/health/db"
      interval: 60s
      timeout: 10s
      expected_status: 200
    
    - name: "redis-health"
      url: "http://backend:8000/api/v1/health/redis"
      interval: 60s
      timeout: 10s
      expected_status: 200

# 性能监控配置
performance:
  # APM配置
  apm:
    service_name: "wlweb"
    environment: "${ENVIRONMENT:-development}"
    server_url: "${APM_SERVER_URL:-http://apm-server:8200}"
    secret_token: "${APM_SECRET_TOKEN}"
  
  # 分布式追踪
  tracing:
    jaeger:
      agent_host: "jaeger-agent"
      agent_port: 6831
      collector_endpoint: "http://jaeger-collector:14268/api/traces"
    
    zipkin:
      endpoint: "http://zipkin:9411/api/v2/spans"
  
  # 性能基准
  benchmarks:
    response_time_p95: 2000  # ms
    response_time_p99: 5000  # ms
    throughput_min: 100      # requests/second
    error_rate_max: 1        # percentage

# 日志监控配置
log_monitoring:
  # Loki配置
  loki:
    url: "http://loki:3100"
    tenant_id: "wlweb"
  
  # 日志告警
  log_alerts:
    - name: "HighErrorLogRate"
      query: 'rate({job="app"} |= "ERROR"[5m])'
      threshold: 0.1
      duration: "2m"
    
    - name: "SecurityAlert"
      query: '{job="security"} |= "SECURITY_VIOLATION"'
      threshold: 0
      duration: "0s"

# 监控仪表板配置
dashboards:
  refresh_interval: "30s"
  time_range: "1h"
  
  panels:
    - title: "System Overview"
      type: "stat"
      targets:
        - expr: "up"
        - expr: "rate(http_requests_total[5m])"
        - expr: "100 - (avg(irate(node_cpu_seconds_total{mode='idle'}[5m])) * 100)"
    
    - title: "Response Time"
      type: "graph"
      targets:
        - expr: "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
        - expr: "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))"
    
    - title: "Error Rate"
      type: "graph"
      targets:
        - expr: "rate(http_requests_total{status=~'5..'}[5m]) / rate(http_requests_total[5m]) * 100"